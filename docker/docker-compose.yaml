version: "3.8"

services:
  vllm:
    build: .
    container_name: vllm_server
    ports:
      - "5000:5000"
    command: ["python3", "-m", "vllm.entrypoints.openai.api_server", "--model", "ibm-granite/granite-guardian-3.1-2b", "--port", "5000"]

  guardian_filter:
    build: .
    container_name: guardian_filter
    ports:
      - "5000:5000"
    depends_on:
      - vllm
    environment:
      - GUARDIAN_API_URL=http://vllm_server:5000
    volumes:
      - ./models:/app/models
    command: ["python3", "models/guardian_filter.py"]

  mitmproxy:
    image: mitmproxy/mitmproxy
    container_name: mitmproxy
    depends_on:
      - guardian_filter
    ports:
      - "8080:8080"
    volumes:
      - ./mitmproxy:/home/mitmproxy
    command: ["mitmdump", "-s", "/home/mitmproxy/proxy.py", "--mode", "transparent", "--listen-host", "0.0.0.0", "--listen-port", "8080"]

  nginx:
    image: nginx:latest
    container_name: nginx_reverse_proxy
    depends_on:
      - mitmproxy
    ports:
      - "443:443"
      - "80:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./certs:/etc/ssl/certs
    entrypoint: ["/bin/bash", "/etc/nginx/entrypoint.sh"]
